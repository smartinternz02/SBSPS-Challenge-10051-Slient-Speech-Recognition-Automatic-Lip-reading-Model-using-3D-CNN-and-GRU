import streamlit as st 
import time 
import asyncio 
from PIL import Image 
import cv2 
from moviepy import editor as moviepy 
import imageio 

import pandas as pd 
import numpy as np 

import os 

from matplotlib import pyplot as plt 
import plotly.express as px 

from utils import load_video, num_to_char 


from streamlit_extras.grid import grid  
from streamlit_option_menu import option_menu 
from streamlit_extras.add_vertical_space import add_vertical_space 

from backend import watson_speech_prediction 



st.set_page_config(layout = "wide", initial_sidebar_state="expanded", page_title = "LipNet - alphaAxon", page_icon="./assets/favicon.png")  

st.session_state["video_path"] = None 

#  Bootstrap CDN
#  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/css/bootstrap.min.css" integrity="sha384-rbsA2VBKQhggwzxH7pPCaAqO46MgnOM80zW1RWuH61DGLwZJEdK2Kadq2F9CUG65" crossorigin="anonymous">
#

with st.sidebar:
    st.image("assets/alphaAxon_final.png", use_column_width = True)  

CDNs = """         
            <link rel="preconnect" href="https://fonts.googleapis.com">
            <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
            <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@100;300;400;600&display=swap" rel="stylesheet">
            <link rel="preconnect" href="https://fonts.googleapis.com">
            <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
            <link href="https://fonts.googleapis.com/css2?family=Orbitron:wght@400;500;600;700&family=Poppins:wght@100;300;400;600&display=swap" rel="stylesheet">
        """

st.markdown(CDNs, unsafe_allow_html=True) 



navbar = option_menu(menu_title = None, 
            options = ["Home", "LipNet Model","About"], 
            icons = ["home", "brain", "search"], 
            default_index = 0, orientation="horizontal")    


if navbar == "Home":

    st.title("A Stunning Approach to Capture Speech") 

    st.markdown("""

        <div class="container">
            <div class="left">
                <h4 class = "space" >Transforming Gestures into Crystal Clear Communication </h4>  
                <h3 class = "space">Uncover the hidden melodies through our captivating Lip reading platform</h3> 
            </div>
            <div class="middle">
                <h3 class = "space">Presenting you the power of <br> <b class = "bold">Computer Vision </b> and <b class = "bold"> Neural Networks </b></h3> 
                <i><h4>"See the Unheard, Understand the Unsaid."</h4></i> 
            </div>
            <div class = "right">
                <h3 class = "space">Unlocking cutting-edge possibilities with Deep Learning</h3>
                <h3 class = "thin">Revolutionizing Silent Speech Analysis for a Connected World</h3>   
            </div>
        </div>

        <div class = "container container-two"> 
                <div class = 'neo'> 
                    <h3>Immense yourself in the muted elegance of Lip-reading</h2>
                    <h4>Open up new possibilities of communication with intelligence.</h4>
                </div>
                <div>
                    <button class = "hero-button">Explore The Speech Model Now! </button>
                </div>
        </div> 
    
    """, unsafe_allow_html = True)  

    with st.sidebar:
        st.write("---") 
        st.markdown("""
            <p>This Technology is Extremely useful in environments where audio isn't optimal for recognition</p> 
            <p>Can be used in Medical Diagnosis for people with disability, who lost their abikity to speak</p> 
            <p>Can be used to improve Human-Computer interface, wearables like augmented reality could open up new possibilities of interaction and control</p> 
        """, unsafe_allow_html=True)  




elif navbar == "LipNet Model": 

    st.sidebar.write("---") 
    st.sidebar.info("A little insight About LipNet")       
    st.sidebar.markdown("""
        <p style = "font-size:1.1rem;">Our model is trained on delicate data with optimal lighting throught the face and face pointing direcly at the camera, Thus as of now it might not be so accurate with every type of video</p> 
        <p style = "font-size:1.1rem;">Also the video should be exactly 75 frames, that's the input shape our model can process</p>
        <p style = "font-size:1.1rem;">Hence we've provided a part of prepeocessed data, which can be used to test the model.</p>
    """, unsafe_allow_html = True)   
    st.sidebar.warning("Hence it's reccomended to use provided dataset for evaluation")  
    st.title("The Ultimate Deep Learning Model to read :lips:")  
    add_vertical_space(1) 

    st.info("Note : Our Model is not so mature, yet we've been constantly working on this to improve its abilities, Thus be kind with it :heart: ", icon="üìù") 

    add_vertical_space(1) 

    gap1,col1, gap2,col2, gap3 = st.columns([1,10,1,10,1])  
    dataset = os.listdir("./Dataset")
    
    

    video_path = col1.selectbox(label = "Select Video from existing Dataset: ", options = ["Select a Video",*dataset]) 

    if(video_path != "Select a Video" and (video_path.endswith(".mp4") or video_path.endswith(".mpg"))): 
        clip = moviepy.VideoFileClip("./Dataset/"+video_path)    
        clip.write_videofile("./temp/test.mp4")  
        col1.video("./temp/test.mp4") 
        st.session_state["video_path"] = "./temp/test.mp4" 

    if(st.session_state.video_path is None):
        col2.markdown("""
            <div class = "box preprocess">
                <h2>Select a Video to Process into tensors</h2> 
                <h3>By feature extraction only lower face will be detected and converted into standardized form</h3>
            </div>
        """,unsafe_allow_html = True) 

    elif st.session_state.video_path:


        frames = load_video(st.session_state.video_path).numpy()  
        #st.write(type(frames)) 
        #st.write(frames)     
        col2.markdown("""
            <h3 style = "font-size:1.7rem; margin-top:2.5rem; margin-bottom:2.5rem; ">This is What the Deep Learning Model Sees! </h3> 
            <!--<h4 style = "font-weight:500; ">Select which frame to View : </h4> -->  
        """, unsafe_allow_html = True)   
        #frame_index = col2.slider("",0,75,20)     
        #df_frames =  pd.DataFrame([range(0,75), np.array([frames.reshape(1,75,46,140)]) ], columns = ["idx", "frame"])  
        #img = px.imshow(df_frames, animation_frame = df_frames["idx"])  
        #img = px.imshow(frames[frame_index].reshape(46,140))   
        #col2.plotly_chart(img)     

        imageio.mimsave("./temp/anim.gif",frames, format = "gif" ,fps = 20)    
        col2.image("./temp/anim.gif", use_column_width=True)    

        # subcol1, subcol2 =  col2.columns(2)
        
        # for frame_index in range(0,35,15): 
        #     img = px.imshow(frames[frame_index].reshape(46,140))     
        #     subcol1.plotly_chart(img, use_container_width = True)      
        #     img = px.imshow(frames[frame_index + 15].reshape(46,140))     
        #     subcol2.plotly_chart(img, use_container_width = True)  



        st.markdown("""
            <h2 class = "center-text" >Prediction From the Model</h2>
        """,unsafe_allow_html=True)
        with st.spinner("Predicting..."): 
            prediction, status = watson_speech_prediction(frames.tolist())  
        if status:
            st.success("Speech SuccessFully Predicted :sparkles: ") 
            st.markdown(f"""
                <h3 class = "center-text">Predicted Words : <b><i>{prediction}</i><b> </h3>  
            """,unsafe_allow_html = True) 
        else:
            st.error(prediction+" Please retry after sometime") 
        st.warning("The Model isn't always 100\% Accurate, that's what makes it more humanly :wink: :smiling_face_with_smiling_eyes_and_hand_covering_mouth: ")    
 


elif navbar == "About":
    st.markdown(f"""
        <div class="about-container">
            <h1>About Us! : Team AlphaAxon</h1><br> 
            <h5>We the students of</h5>
            <h3>Dr. M.G.R Educational and Research Institute</h3> 
            <h5>Maduravoyal, Chennai, TamilNadu, India</h5> 
            <h3 style = "margin-top:1.5rem; ">Have done this Project under IBM Hack Challenge 2023</h3>
            <h4>Silent Speech Recognition with CNN, GRU and Computer Vision.</h4><br> 
            <div class = "about_info"><span class = "">This Model is built with <b>Tensorflow</b> and trained by the <b>GRID dataset</b>,<br>which consists of videos and annotations of people uttering few words, and is Hosted by <br> <b>IBM Watson Machine Learning</b> and <b>IBM Cloud</b></span></div>  
            <div class = "footer">
                <div>
                <p>Built by :</p>   
                <p>Gurubaravikkram K R</p>
                <p>Dinesh Kumar S</p>
                <p>Hemanth Kumar M</p>
                <p>Harishwar J</p> 
                </div> 
            </div> 
        </div>
        
""", unsafe_allow_html = True) 
    
    with st.sidebar:
        st.markdown("""
            <p style = "font-size:1.2rem; margin-bottom:2rem; " >This Model is Based Upon the LipNet paper which contains of implementation of silent speech recognition using only visual data. </p><br> 
            <p style = "font-size:1.1rem">Various Deep Learning Techniques Such as Convolutional 3D Neural Networks and GRU are used to construct the model. </p>
        """, unsafe_allow_html=True)   
        

# with open("style.css", 'r') as f:
#     styles = f"""<style>{f.readlines()}</style>"""  
#     st.markdown(styles, unsafe_allow_html=True)   

styles = """
    <style> 

        *{
            font-family:"Source Sans Pro", sans-serif; 
         }

        h1{
            display:block; 
            text-align:center; 
            font-weight:500; 
            letter-spacing:0.4rem;  
        }

        .space{
            line-height:2; 
        }

        .thin{
            font-weight:500; 
        }


        *{
            font-family:"Monospace", sans-serif; 
        }

        #app{
            font-size:3rem;  
        }

        .logo{
            width:100%; 
            height:auto; 
            display:flex; 
            justify-content:center; 
            align-items:center; 
        }

        .logo img{
            height: auto; 
            width: 350px; 
        }

        header, footer{
            visibility:hidden;        
        }

        .justify-center{
            display:flex; 
            flex-direction:row; 
            justify-content:center; 
        }

        div .block-container{ 
            padding:2rem; 
            padding-top:1rem; 
        }

        .container{
            width: 100%; 
            display: flex; 
            flex-direction:row; 
            justify-content:center; 
            margin-top:3rem; 
            gap:2rem; 
            text-align:center; 
            flex-wrap:wrap; 
        }

        .center-text{
            text-align:center; 
        }

        .hero-button{
            padding:5px 15px; 
            outline:none; 
            border:none; 
            border-radius:5px; 
            font-size:1.2rem; 
            background: #12c2e9;  /* fallback for old browsers */
            background: -webkit-linear-gradient(to right, #c471ed, #12c2e9);  /* Chrome 10-25, Safari 5.1-6 */
            background: linear-gradient(to right, #c471ed, #12c2e9); /* W3C, IE 10+/ Edge, Firefox 16+, Chrome 26+, Opera 12+, Safari 7+ */

        }

        .left, .middle, .right{
            width:30%;
            line-height:5px ; 
            letter-spacing:2px; 
            border-radius:2rem; 
            padding: 1rem 2.5rem; 
            box-shadow:5px 5px 10px 0px rgba(0,0,0,0.5); 
            min-width:300px; 
        }

        .left{
            background: #ee9ca7;  /* fallback for old browsers */
            background: -webkit-linear-gradient(to right, #ffdde1, #ee9ca7);  /* Chrome 10-25, Safari 5.1-6 */
            background: linear-gradient(to right, #ffdde1, #ee9ca7);
        }

        .middle{
            
            background: #757F9A;  /* fallback for old browsers */
            background: -webkit-linear-gradient(to right, #D7DDE8, #757F9A);  /* Chrome 10-25, Safari 5.1-6 */
            background: linear-gradient(to right, #D7DDE8, #757F9A); /* W3C, IE 10+/ Edge, Firefox 16+, Chrome 26+, Opera 12+, Safari 7+ */

        }

        .right{
            background: #36D1DC;  /* fallback for old browsers */
            background: -webkit-linear-gradient(to right, #5B86E5, #36D1DC);  /* Chrome 10-25, Safari 5.1-6 */
            background: linear-gradient(to right, #5B86E5, #36D1DC); /* W3C, IE 10+/ Edge, Firefox 16+, Chrome 26+, Opera 12+, Safari 7+ */

        }

        h3{
            font-weight:500; 
        }

        h4{
            font-weight:600; 
        }

        .bold{
            font-weight:700; 
        }

        .container-two{
            padding-top:2rem; 
            align-items:center; 
            gap:4rem;  
        }

        .container-two .neo{
            padding:1rem; 2rem;  
            border-radius:5px; 
            box-shadow:2px 2px 5px 2px rgba(0,0,0,0.3),  
                        -5px -5px 5px 0px rgba(255,255,255,.5), 
                        -2px -2px 5px 5px rgba(255,255,255,0.5);   
        }

        .about-container{
            display:flex; 
            flex-direction:column; 
            text-align:center; 
            margin-top:3rem; 
            align-items:center; 
        }

        .about-container h1{ 
            letter-spacing:2px; 
            font-weight:700; 
            font-family:"Poppins",sans-serif; 
        }

        .info{
            background:rgba(0,200,200,.4); 
            padding:1rem 2rem; 
            border: 2px solid lightblue;  
            border-radius:10px; 
            width:max-content;  
        }

        .about_info{
            font-weight:500; 
            line-height:2; 
            padding:2rem 2.5rem; 
            border-radius:10px;  
            font-size:1.3rem;  
            width:max-content; 
            margin-bottom:3rem; 
            background: #FFAFBD;  /* fallback for old browsers */
            background: -webkit-linear-gradient(to right, #ffc3a0, #FFAFBD);  /* Chrome 10-25, Safari 5.1-6 */
            background: linear-gradient(to right, #ffc3a0, #FFAFBD); /* W3C, IE 10+/ Edge, Firefox 16+, Chrome 26+, Opera 12+, Safari 7+ */

            color:black; 
            box-shadow:5px 5px 10px 0px rgba(0,0,0,0.5);  
        }

        .about-container .footer{
            width:100%; 
            display:flex; 
            justify-content:center;    
            align-items:center; 
            gap:2rem; 
            margin:2rem;  
            margin-right:3rem;    
           
        }

        .footer div{
            display:flex; 
            align-items:center; 
            justify-content:center; 
            padding:0.75rem 1.5rem;  
            gap:2rem; 
            background:#353b48;  
            color:white; 
            font-weight:700;  
            border-radius:1rem; 
        }

        .about-container .footer p{
            font-size:1.5rem; 
            text-transform:uppercase; 
            margin:0;   
        }

        .preprocess{
            display:flex; 
            flex-direction:column; 
            justify-content:center; 
        }

    </style> 

"""

logo_container = """
    <div class = "logo"> 
        <img  src="resized.png" ></img>   
    </div>
"""

st.markdown(styles, unsafe_allow_html=True)  


#my_grid = grid(1,[1,1], vertical_align = "top")     


    
    #first, center, last = st.columns([1,2,1])   
    #st.image(logo, use_column_width=True) 
    # st.markdown(logo_container, unsafe_allow_html=True)  
    # upload_file = st.file_uploader("Upload Video file to detect Words", type = ["mpg"])   
      
      




# def get_video(FILE_NAME): 
#     cap = cv2.VideoCapture(FILE_NAME) 
#     frames = [] 
#     image_container = st.image([])  

#     while True:  
#         status, frame = cap.read()

#         if not status:
#             break

#         # if not status:
#         #     cap = cv2.VideoCapture(FILE_NAME) 
#         #     status, frame = cap.read() 
          
#         img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  
#         frames.append(img)   
#         # image_container.image(img)
    
#     return frames

# def process_video(frames):
#     processed = []  
#     for frame in frames:
#        processed.append(tf.image.rgb_to_grayscale(frame))    
#     return ( tf.cast(processed - tf.math.reduce_mean(processed), tf.float32) / tf.math.reduce_std(tf.cast(processed, tf.float32)))    
      


# run = st.checkbox("Run Video") 

# if run:
#     frames = get_video("test_4.mp4")

#     print("#"*10, "\n"*5)  
#     print(np.array(frames).shape)   
#     print(np.array(frames).dtype)    
#     print("#"*10, "\n"*5)

#     frames = tf.cast(process_video(frames)[:75], tf.int8)   

#     print("#"*10, "\n"*5)  
#     print(frames.shape)  
#     print("#"*10, "\n"*5)  
#     print(frames)   
    

#     #imageio.mimsave("animation.gif",frames, duration=20)  
#     image_container = st.image([])  
#     while run:
#         for i in frames.numpy(): 
#             image_container.image(i)  
#     # st.image("animation.gif", width = 400)      




# st.snow() 

# st.balloons() 







